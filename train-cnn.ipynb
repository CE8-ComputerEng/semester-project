{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "from includes.dataset import SpectrogramDataset\n",
    "import includes.utils as utils\n",
    "import includes.cnn_dataimporter as cnn_dataimporter\n",
    "import matplotlib.pyplot as plt\n",
    "from includes.data_splitter import split_data_val_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define class DistributionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributionDataset:\n",
    "    def __init__(self, data_np, labels_np, CLASSES):\n",
    "        label_count = np.zeros(len(CLASSES))\n",
    "        for data_np, labels_np in zip(data_np, labels_np):\n",
    "            label_count += labels_np.label\n",
    "        self.index = CLASSES\n",
    "        self.values = label_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible spectrogram types are \"stft\", \"mel\", \"mfcc\", and \"pncc\"\n",
    "spectrogram_type = \"mel\"\n",
    "CLASSES = ['JUMP','BOAT','SEAGUL_SCREAM','BRIDGE','SCOOTER','PEE','OBJECT_SPLASH','UFO','IDLE_MOTOR','SEAGUL_SPLASH','VOICE', 'SWIM']\n",
    "FULL_TRAIN_DATASET_PATH = 'data/full_data/'+spectrogram_type+'/train_data.npy'\n",
    "FULL_TRAIN_LABEL_PATH = 'data/full_data/'+spectrogram_type+'/train_labels.npy'\n",
    "FULL_VAL_DATASET_PATH = 'data/full_data/'+spectrogram_type+'/val_data.npy'\n",
    "FULL_VAL_LABEL_PATH = 'data/full_data/'+spectrogram_type+'/val_labels.npy'\n",
    "\n",
    "# Check if the full dataset has already been created\n",
    "if not os.path.exists(FULL_TRAIN_DATASET_PATH) or not os.path.exists(FULL_TRAIN_LABEL_PATH) or not os.path.exists(FULL_VAL_DATASET_PATH) or not os.path.exists(FULL_VAL_LABEL_PATH) :\n",
    "    # Load the data and labels\n",
    "\n",
    "    TRAIN_NPDATAPATH = 'data/training/'+spectrogram_type+'/data.npy'\n",
    "    TRAIN_NPLABELPATH = 'data/training/'+spectrogram_type+'/labels.npy'\n",
    "    TRAIN_DATAPATH = 'data/training/clips/wav/'\n",
    "    TRAIN_LABELPATH = 'data/training/clips/txt/'\n",
    "    TRAIN_TIMEPATH = 'data/training/clips/time/'\n",
    "    train_data_np, train_labels_np, train_data_size = cnn_dataimporter.import_data(TRAIN_NPDATAPATH, TRAIN_NPLABELPATH, TRAIN_DATAPATH, TRAIN_LABELPATH, TRAIN_TIMEPATH, CLASSES, spectrogram_type)\n",
    "\n",
    "    TEST_NPDATAPATH = 'data/test/'+spectrogram_type+'/data_jump.npy'\n",
    "    TEST_NPLABELPATH = 'data/test/'+spectrogram_type+'/labels_jump.npy'\n",
    "    TEST_DATAPATH = 'data/test/clips/wav/'\n",
    "    TEST_LABELPATH = 'data/test/clips/txt/'\n",
    "    TEST_TIMEPATH = 'data/test/clips/time/'\n",
    "    # Plot the distribution of the labels\n",
    "    test_data_np, test_labels_np, test_data_size = cnn_dataimporter.import_data(TEST_NPDATAPATH, TEST_NPLABELPATH, TEST_DATAPATH, TEST_LABELPATH,TEST_TIMEPATH, CLASSES, spectrogram_type)\n",
    "    \n",
    "    # Concatenate the test data and train data\n",
    "    train_data_np = np.concatenate((train_data_np, test_data_np), axis=0)\n",
    "    train_labels_np = np.concatenate((train_labels_np, test_labels_np), axis=0)\n",
    "    print(\"train_data_np.shape = \", train_data_np.shape)\n",
    "    print(\"train_labels_np.shape = \", train_labels_np.shape)\n",
    "\n",
    "    c = 0\n",
    "    #for single_class, i in zip(CLASSES, range(len(CLASSES))):\n",
    "    for label in test_labels_np:\n",
    "        # check if a clip only contains IDLE_MOTOR\n",
    "        if label.label[10] == 1:\n",
    "            if label.label[2]:\n",
    "                if label.label.sum() == 2:\n",
    "                    c += 1\n",
    "    print(\"Number of IDLE_MOTOR + BAOT clips = \", c)\n",
    "    c = 0\n",
    "        \n",
    "    distribution = DistributionDataset(train_data_np, train_labels_np, CLASSES)\n",
    "    print(\"Training data value distribution = \", distribution.values)\n",
    "    #jump_distribution = DistributionDataset(test_data_np, test_labels_np, CLASSES)\n",
    "\n",
    "\n",
    "    # ## Prepare Training, validation and test data\n",
    "    TRAINING_RATIO = 0.8\n",
    "    VALIDATION_RATIO = 0.1\n",
    "    TEST_RATIO = 0.1\n",
    "\n",
    "    if TRAINING_RATIO + VALIDATION_RATIO + TEST_RATIO != 1:\n",
    "        raise ValueError('Training, validation, and test ratios must sum to 1.')\n",
    "\n",
    "    train_spectrograms, train_labels, val_spectrograms, val_labels = split_data_val_train(train_data_np, train_labels_np, CLASSES, TRAIN_MAX_SAMPLES_PER_CLASS=4000)\n",
    "    # Save the full dataset and labels\n",
    "    np.save(FULL_TRAIN_DATASET_PATH, train_spectrograms)\n",
    "    np.save(FULL_TRAIN_LABEL_PATH, train_labels)\n",
    "    np.save(FULL_VAL_DATASET_PATH, val_spectrograms)\n",
    "    np.save(FULL_VAL_LABEL_PATH, val_labels)\n",
    "else:\n",
    "    train_spectrograms = np.load(FULL_TRAIN_DATASET_PATH)\n",
    "    train_labels = np.load(FULL_TRAIN_LABEL_PATH, allow_pickle=True)\n",
    "    val_spectrograms = np.load(FULL_VAL_DATASET_PATH)\n",
    "    val_labels = np.load(FULL_VAL_LABEL_PATH, allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_distribution = DistributionDataset(train_spectrograms, train_labels, CLASSES)\n",
    "print(\"Training data value distribution after split = \", training_distribution.values)\n",
    "#utils.plot_label_distribution(training_distribution, filename='train_distribution.png')\n",
    "\n",
    "validation_distribution = DistributionDataset(val_spectrograms, val_labels, CLASSES)\n",
    "print(\"Validation data value distribution = \", validation_distribution.values)\n",
    "#utils.plot_label_distribution(validation_distribution, filename='val_distribution.png')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transforms for data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Add more transforms here\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Balance the training data. This can be done by using the pyto4rch WeightedRandomSampler\n",
    "# http://pytorch.org/docs/data.html#torch.utils.data.sampler.WeightedRandomSampler\n",
    "class_sample_count = training_distribution.values\n",
    "weights = 1. / torch.Tensor(class_sample_count)\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, BATCH_SIZE)\n",
    "\n",
    "\n",
    "# ## Make datasets\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = SpectrogramDataset(train_spectrograms, train_labels, transform=transform)\n",
    "val_dataset = SpectrogramDataset(val_spectrograms, val_labels, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classes, and summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cnn_models.cnn_model import CNNClassifier\n",
    "print(train_spectrograms[0].shape)\n",
    "train_data_size = train_spectrograms[0].shape\n",
    "model = CNNClassifier(classes=CLASSES, sample_shape=train_spectrograms[0].shape, spectrogram_type=spectrogram_type)\n",
    "print(\"Data size: \", train_data_size)\n",
    "\n",
    "sum = summary(model, (1 , train_data_size[0], train_data_size[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 40\n",
    "VERSION = 'huge_cnn_v2_epoch-'+str(MAX_EPOCHS)+'-multi_label__multi_ch_medium_clip_with_all_jumps_sigmoid'+'_'+spectrogram_type + '_custom_loss'\n",
    "\n",
    "accelerator = None\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = 'gpu'\n",
    "elif torch.backends.mps.is_available():\n",
    "    accelerator = 'cpu'  # MPS is not implemented in PyTorch yet\n",
    "\n",
    "tb_logger = loggers.TensorBoardLogger('.', version=VERSION)\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_acc', mode='max', save_top_k=1, save_last=True, filename='best-{epoch}-{val_acc:.2f}')\n",
    "\n",
    "trainer = Trainer(accelerator=accelerator, max_epochs=MAX_EPOCHS, logger=tb_logger, callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(accelerator=accelerator)\n",
    "# Load previously trained model\n",
    "\n",
    "# Best MEL = best-epoch=16-val_acc=0.55.ckpt\n",
    "# Best MFCC = best-epoch=30-val_acc=0.51.ckpt\n",
    "# BEST STFT = best-epoch=36-val_acc=0.63.ckpt\n",
    "CHECKPOINT_PATH = f'lightning_logs/{VERSION}/checkpoints/best-epoch=36-val_acc=0.53.ckpt'\n",
    "\n",
    "model = CNNClassifier.load_from_checkpoint(CHECKPOINT_PATH, classes=CLASSES, sample_shape=train_spectrograms[0].shape, spectrogram_type=spectrogram_type)\n",
    "print(f'Model size: {os.path.getsize(CHECKPOINT_PATH) / 1e6} MB')\n",
    "\n",
    "stats = trainer.test(model, val_loader)\n",
    "# Save the stats in a txt file\n",
    "with open(f'CNN/stats_'+spectrogram_type+'.txt', 'w') as f:\n",
    "    f.write(str(stats))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
