{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torchmetrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "import glob\n",
    "import tqdm\n",
    "from dataset import SpectrogramDataset\n",
    "import utils\n",
    "from utils import plot_confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(pl.LightningModule):\n",
    "    def __init__(self, classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        self.classes = classes\n",
    "        num_classes = len(classes)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 2 * 6, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.confusion_matrix = torchmetrics.ConfusionMatrix(num_classes=num_classes, task='multiclass')\n",
    "        \n",
    "        self.test_confusion_matrix = np.zeros((num_classes, num_classes))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = x.view(-1, 128 * 2 * 6)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        \n",
    "        _, predicted = torch.max(y_hat, 1)\n",
    "        accuracy = (predicted == y).sum().item() / len(y)\n",
    "        self.log('train_acc', accuracy, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "        _, predicted = torch.max(y_hat, 1)\n",
    "        accuracy = (predicted == y).sum().item() / len(y)\n",
    "        self.log('val_acc', accuracy)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        \n",
    "        _, predicted = torch.max(y_hat, 1)\n",
    "        accuracy = (predicted == y).sum().item() / len(y)\n",
    "        self.log('test_acc', accuracy)\n",
    "        \n",
    "        confusion_matrix = self.confusion_matrix(predicted, y)\n",
    "        self.test_confusion_matrix += confusion_matrix.cpu().numpy()\n",
    "        \n",
    "    def test_epoch_end(self, outputs):\n",
    "        plot_confusion_matrix(self.test_confusion_matrix, self.classes, filename='confusion_matrix.png')\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        x = batch\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        _, predicted = torch.max(y_hat, 1)\n",
    "        return predicted\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPDATAPATH = 'data/measurment-2/data.npy'\n",
    "NPLABELPATH = 'data/measurment-2/labels.npy'\n",
    "# Try to load the data from the numpy file\n",
    "try:\n",
    "    data_np = np.load(NPDATAPATH)\n",
    "    data_size = (data_np.shape[2], data_np.shape[3])\n",
    "    labels_np = np.load(NPLABELPATH)\n",
    "    \n",
    "    \n",
    "    \n",
    "except:\n",
    "    # If the file doesn't exist, create it.\n",
    "    DATAPATH = 'data/measurment-2/clips/wav/'\n",
    "    LABELPATH = 'data/measurment-2/clips/txt/'\n",
    "\n",
    "    data = glob.glob(DATAPATH + '*.wav')\n",
    "    labels = glob.glob(LABELPATH + '*.txt')\n",
    "\n",
    "    # Convert each wav file to a spectrogram and save it in a numpy array\n",
    "    #data_np = np.empty((len(data)))\n",
    "    #labels_np = np.empty((len(labels)))\n",
    "    data_np = []\n",
    "    labels_np = []\n",
    "    # Use tqdm to show progress bar\n",
    "    pbar = tqdm.tqdm(total=len(data))\n",
    "\n",
    "    # Only test on 10 files for now\n",
    "    #data = data[:100]\n",
    "    i = 0\n",
    "    for file, label in zip(data, labels):\n",
    "        audio_file, rate_of_sample = torchaudio.load(file)\n",
    "        spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=rate_of_sample, n_mels=128, n_fft=2048)(audio_file)\n",
    "        spectrogram = torchaudio.transforms.AmplitudeToDB()(np.abs(spectrogram))\n",
    "        data_np.append(spectrogram)\n",
    "        labels_np.append(label)\n",
    "        i += 1\n",
    "        \"\"\"if data_np.size == 0:\n",
    "            data_np[i] = spectrogram\n",
    "            labels_np[i] = label\n",
    "        else:\n",
    "            data_np = np.concatenate((data_np, spectrogram), axis=0)\n",
    "            labels_np = np.concatenate((labels_np, label), axis=0)\"\"\"\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    # Convert list of numpy arrays to a single numpy array\n",
    "    data_np = np.stack(data_np)\n",
    "    labels_np = np.stack(labels_np)\n",
    "    data_size = (data_np.shape[2], data_np.shape[3])\n",
    "    print('Data shape:', data_np.shape)\n",
    "    print('Labels shape:', labels_np.shape)\n",
    "    # Save the numpy array\n",
    "    np.save('data/measurment-2/data.npy', data_np)\n",
    "    np.save('data/measurment-2/labels.npy', labels_np)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just for fun: spectrogram plot loop\n",
    "Uncomment if you want to see the plot of the spectrograms in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%matplotlib qt\n",
    "#utils.loop_plot_audio_spectogram(data_np)\n",
    "#%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 3089\n",
      "Validation samples: 386\n",
      "Test samples: 387\n"
     ]
    }
   ],
   "source": [
    "TRAINING_RATIO = 0.8\n",
    "VALIDATION_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "if TRAINING_RATIO + VALIDATION_RATIO + TEST_RATIO != 1:\n",
    "    raise ValueError('Training, validation, and test ratios must sum to 1.')\n",
    "\n",
    "train_size = int(TRAINING_RATIO * len(data_np))\n",
    "val_size = int(VALIDATION_RATIO * len(data_np))\n",
    "test_size = len(data_np) - train_size - val_size\n",
    "\n",
    "train_spectrograms, val_spectrograms, train_labels, val_labels = train_test_split(data_np, labels_np, test_size=val_size, random_state=42)\n",
    "train_spectrograms, test_spectrograms, train_labels, test_labels = train_test_split(train_spectrograms, train_labels, test_size=test_size, random_state=42)\n",
    "\n",
    "print('Training samples:', train_spectrograms.shape[0])\n",
    "print('Validation samples:', val_spectrograms.shape[0])\n",
    "print('Test samples:', test_spectrograms.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transforms for data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Add more transforms here\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = SpectrogramDataset(train_spectrograms, train_labels, transform=transform)\n",
    "val_dataset = SpectrogramDataset(val_spectrograms, val_labels, transform=transforms.ToTensor())\n",
    "test_dataset = SpectrogramDataset(test_spectrograms, test_labels, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classes, and summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size:  (128, 94)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchsummary. See above stack traces for more details. Executed layers up to: [Conv2d: 1-1, ReLU: 1-2, MaxPool2d: 1-3, Conv2d: 1-4, ReLU: 1-5, MaxPool2d: 1-6, Conv2d: 1-7, ReLU: 1-8, MaxPool2d: 1-9, Conv2d: 1-10, ReLU: 1-11, MaxPool2d: 1-12]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32md:\\OneDrive\\Github\\CE8-semester-project\\.venv\\lib\\site-packages\\torchsummary\\torchsummary.py:140\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_data, batch_dim, branching, col_names, col_width, depth, device, dtypes, verbose, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 140\u001b[0m         _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)(\u001b[39m*\u001b[39mx, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\OneDrive\\Github\\CE8-semester-project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 36\u001b[0m, in \u001b[0;36mCNNClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[1;32m---> 36\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m128\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m6\u001b[39;49m)\n\u001b[0;32m     38\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 1536]' is invalid for input of size 10240",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[39m=\u001b[39m CNNClassifier(classes\u001b[39m=\u001b[39mCLASSES)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mData size: \u001b[39m\u001b[39m\"\u001b[39m, data_size)\n\u001b[1;32m----> 6\u001b[0m summary(model, (\u001b[39m1\u001b[39;49m, data_size[\u001b[39m0\u001b[39;49m], data_size[\u001b[39m1\u001b[39;49m]))\n",
      "File \u001b[1;32md:\\OneDrive\\Github\\CE8-semester-project\\.venv\\lib\\site-packages\\torchsummary\\torchsummary.py:143\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_data, batch_dim, branching, col_names, col_width, depth, device, dtypes, verbose, *args, **kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    142\u001b[0m     executed_layers \u001b[39m=\u001b[39m [layer \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m summary_list \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mexecuted]\n\u001b[1;32m--> 143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    144\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to run torchsummary. See above stack traces for more details. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExecuted layers up to: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(executed_layers)\n\u001b[0;32m    146\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m hooks \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to run torchsummary. See above stack traces for more details. Executed layers up to: [Conv2d: 1-1, ReLU: 1-2, MaxPool2d: 1-3, Conv2d: 1-4, ReLU: 1-5, MaxPool2d: 1-6, Conv2d: 1-7, ReLU: 1-8, MaxPool2d: 1-9, Conv2d: 1-10, ReLU: 1-11, MaxPool2d: 1-12]"
     ]
    }
   ],
   "source": [
    "CLASSES = ['Background', 'Anomaly']\n",
    "\n",
    "model = CNNClassifier(classes=CLASSES)\n",
    "print(\"Data size: \", data_size)\n",
    "\n",
    "summary(model, (1, data_size[0], data_size[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 30\n",
    "VERSION = 'cnn_v2_epoch-30-v2'\n",
    "\n",
    "accelerator = None\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = 'gpu'\n",
    "elif torch.backends.mps.is_available():\n",
    "    accelerator = 'cpu'  # MPS is not implemented in PyTorch yet\n",
    "\n",
    "tb_logger = loggers.TensorBoardLogger('.', version=VERSION)\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_acc', mode='max', save_top_k=1, save_last=True, filename='best-{epoch}-{val_acc:.2f}')\n",
    "\n",
    "trainer = Trainer(accelerator=accelerator, max_epochs=MAX_EPOCHS, logger=tb_logger, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously trained model\n",
    "CHECKPOINT_PATH = f'lightning_logs/{VERSION}/checkpoints/best-epoch=7-val_acc=0.87.ckpt'\n",
    "\n",
    "model = CNNClassifier.load_from_checkpoint(CHECKPOINT_PATH, classes=CLASSES)\n",
    "print(f'Model size: {os.path.getsize(CHECKPOINT_PATH) / 1e6} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, test_loader) # This not the challenge, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrograms_to_predict = np.load('data/test.npy')\n",
    "\n",
    "print('Test spectrogram to predict shape:', spectrograms_to_predict.shape)\n",
    "print('Test spectrogram to predict dtype:', spectrograms_to_predict.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_predict_dataset = SpectrogramDataset(spectrograms_to_predict, transform=transforms.ToTensor())\n",
    "test_to_predict_loader = DataLoader(test_to_predict_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(model, test_to_predict_loader)\n",
    "predictions = np.concatenate(predictions).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('predictions.txt', predictions, delimiter='\\n', fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
